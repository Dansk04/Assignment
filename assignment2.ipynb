{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emails\n",
    "    email_pattern = re.compile(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\")\n",
    "    text = re.sub(email_pattern, '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text().strip()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(\" \")])\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text\n",
    "tokenized_text = [review.split() for review in data['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec Model - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokenized text\n",
    "model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Convert the tokenized text to vectors using the trained Word2Vec model\n",
    "X = np.array([np.mean([model.wv[word] for word in review], axis=0) for review in tokenized_text])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_cbow, X_test_cbow, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec Model - Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokenized text\n",
    "model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Convert the tokenized text to vectors using the trained Word2Vec model\n",
    "X = np.array([np.mean([model.wv[word] for word in review], axis=0) for review in tokenized_text])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_skip_gram, X_test_skip_gram, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and run Logistic Regression Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model on the training data\n",
    "def logreg(X_train, X_test):\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and run Random Forrest Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranforr(X_train, X_test):\n",
    "    # Train a random forest classifier on the training data\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the logistic regression model on the testing data\n",
    "def evaluate(y_test_cbow, y_test_skip_gram):\n",
    "    print(\"Accuracy-CBOW:\", accuracy_score(y_test, y_test_cbow))\n",
    "    print(\"Accuracy-Skip-gram:\", accuracy_score(y_test, y_test_skip_gram))\n",
    "    print(\"Precision-CBOW:\", precision_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"Precision-Skip-gram:\", precision_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    print(\"Recall-CBOW:\", recall_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"Recall-Skip-gram:\", recall_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    print(\"F1 Score-CBOW:\", f1_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"F1 Score-Skip-gram:\", f1_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    # Create a confusion matrix\n",
    "    print(\"Confusion Matrix-CBOW\", confusion_matrix(y_test, y_test_cbow))\n",
    "    print(\"Confusion Matrix-Skip-gram\", confusion_matrix(y_test, y_test_skip_gram))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cbow = logreg(X_train_cbow, X_test_cbow)\n",
    "y_test_skip_gram = logreg(X_train_skip_gram, X_test_skip_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-CBOW: 0.4984\n",
      "Accuracy-Skip-gram: 0.8678\n",
      "Precision-CBOW: 0.49635467980295567\n",
      "Precision-Skip-gram: 0.8595870206489675\n",
      "Recall-CBOW: 0.5059248845149629\n",
      "Recall-Skip-gram: 0.8778871259289014\n",
      "F1 Score-CBOW: 0.5010940919037199\n",
      "F1 Score-Skip-gram: 0.8686406995230526\n",
      "Confusion Matrix-CBOW [[2465 2556]\n",
      " [2460 2519]]\n",
      "Confusion Matrix-Skip-gram [[4307  714]\n",
      " [ 608 4371]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_cbow, y_test_skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cbow = ranforr(X_train_cbow, X_test_cbow)\n",
    "y_test_skip_gram = ranforr(X_train_skip_gram, X_test_skip_gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-CBOW: 0.4977\n",
      "Accuracy-Skip-gram: 0.8356\n",
      "Precision-CBOW: 0.4953429297205758\n",
      "Precision-Skip-gram: 0.827667518176459\n",
      "Recall-CBOW: 0.4699738903394256\n",
      "Recall-Skip-gram: 0.8459530026109661\n",
      "F1 Score-CBOW: 0.48232505410697724\n",
      "F1 Score-Skip-gram: 0.836710369487485\n",
      "Confusion Matrix-CBOW [[2637 2384]\n",
      " [2639 2340]]\n",
      "Confusion Matrix-Skip-gram [[4144  877]\n",
      " [ 767 4212]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_cbow, y_test_skip_gram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
