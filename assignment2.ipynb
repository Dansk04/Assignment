{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emails\n",
    "    email_pattern = re.compile(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\")\n",
    "    text = re.sub(email_pattern, '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text().strip()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(\" \")])\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text\n",
    "tokenized_text = [review.split() for review in data['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec Model - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokenized text\n",
    "model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Convert the tokenized text to vectors using the trained Word2Vec model\n",
    "X = np.array([np.mean([model.wv[word] for word in review], axis=0) for review in tokenized_text])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_cbow, X_test_cbow, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec Model - Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokenized text\n",
    "model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Convert the tokenized text to vectors using the trained Word2Vec model\n",
    "X = np.array([np.mean([model.wv[word] for word in review], axis=0) for review in tokenized_text])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_skip_gram, X_test_skip_gram, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and run Logistic Regression Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model on the training data\n",
    "def logreg(X_train, X_test):\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and run Random Forrest Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranforr(X_train, X_test):\n",
    "    # Train a random forest classifier on the training data\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and run Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supvecmach(X_train, X_test):\n",
    "    # Train a support vector machine classifier on the training set\n",
    "    classifier = SVC(random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the logistic regression model on the testing data\n",
    "def evaluate(y_test_cbow, y_test_skip_gram):\n",
    "    print(\"Accuracy-CBOW:\", accuracy_score(y_test, y_test_cbow))\n",
    "    print(\"Accuracy-Skip-gram:\", accuracy_score(y_test, y_test_skip_gram))\n",
    "    print(\"Precision-CBOW:\", precision_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"Precision-Skip-gram:\", precision_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    print(\"Recall-CBOW:\", recall_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"Recall-Skip-gram:\", recall_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    print(\"F1 Score-CBOW:\", f1_score(y_test, y_test_cbow, pos_label='positive'))\n",
    "    print(\"F1 Score-Skip-gram:\", f1_score(y_test, y_test_skip_gram, pos_label='positive'))\n",
    "    # Create a confusion matrix\n",
    "    print(\"Confusion Matrix-CBOW\", confusion_matrix(y_test, y_test_cbow))\n",
    "    print(\"Confusion Matrix-Skip-gram\", confusion_matrix(y_test, y_test_skip_gram))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y_test_cbow = logreg(X_train_cbow, X_test_cbow)\n",
    "y_test_skip_gram = logreg(X_train_skip_gram, X_test_skip_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-CBOW: 0.503\n",
      "Accuracy-Skip-gram: 0.8765\n",
      "Precision-CBOW: 0.5069354139575206\n",
      "Precision-Skip-gram: 0.8765113974231913\n",
      "Recall-CBOW: 0.4646404449741756\n",
      "Recall-Skip-gram: 0.8784266984505363\n",
      "F1 Score-CBOW: 0.4848673300165837\n",
      "F1 Score-Skip-gram: 0.8774680027780533\n",
      "Confusion Matrix-CBOW [[2691 2275]\n",
      " [2695 2339]]\n",
      "Confusion Matrix-Skip-gram [[4343  623]\n",
      " [ 612 4422]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_cbow, y_test_skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulation CBOW vs skip-gram using logistic regression:\n",
    "As we can see, the performance of skip-gram is significantly better than the performance of CBOW. Almost 3700 more texts are correctly assigned to their respective sentiment category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cbow = ranforr(X_train_cbow, X_test_cbow)\n",
    "y_test_skip_gram = ranforr(X_train_skip_gram, X_test_skip_gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-CBOW: 0.4975\n",
      "Accuracy-Skip-gram: 0.8439\n",
      "Precision-CBOW: 0.5009857612267251\n",
      "Precision-Skip-gram: 0.842167487684729\n",
      "Recall-CBOW: 0.45431068732618196\n",
      "Recall-Skip-gram: 0.8490266189908622\n",
      "F1 Score-CBOW: 0.4765079695801646\n",
      "F1 Score-Skip-gram: 0.845583143733307\n",
      "Confusion Matrix-CBOW [[2688 2278]\n",
      " [2747 2287]]\n",
      "Confusion Matrix-Skip-gram [[4165  801]\n",
      " [ 760 4274]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_cbow, y_test_skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulation CBOW vs skip-gram using Random Forrest Performance:\n",
    "As we can see, the performance of skip-gram is significantly better than the performance of CBOW. Almost 3500 more texts are correctly assigned to their respective sentiment category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cbow = supvecmach(X_train_cbow, X_test_cbow)\n",
    "y_test_skip_gram = supvecmach(X_train_skip_gram, X_test_skip_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-CBOW: 0.5045\n",
      "Accuracy-Skip-gram: 0.8767\n",
      "Precision-CBOW: 0.5112953960537604\n",
      "Precision-Skip-gram: 0.8744827586206897\n",
      "Recall-CBOW: 0.35518474374255066\n",
      "Recall-Skip-gram: 0.8816050854191498\n",
      "F1 Score-CBOW: 0.4191771187434064\n",
      "F1 Score-Skip-gram: 0.8780294786823623\n",
      "Confusion Matrix-CBOW [[3257 1709]\n",
      " [3246 1788]]\n",
      "Confusion Matrix-Skip-gram [[4329  637]\n",
      " [ 596 4438]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test_cbow, y_test_skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulation CBOW vs skip-gram using Support Vector Machine:\n",
    "As we can see, the performance of skip-gram is significantly better than the performance of CBOW. Almost 3700 more texts are correctly assigned to their respective sentiment category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
